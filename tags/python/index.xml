<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on Johnny's Blog</title><link>https://lijohnny.com/tags/python/</link><description>Recent content in Python on Johnny's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 14 Mar 2021 22:18:37 -0700</lastBuildDate><atom:link href="https://lijohnny.com/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>Price Strategy Based on Features Reputation</title><link>https://lijohnny.com/p/price-strategy-based-on-features-reputation/</link><pubDate>Sun, 14 Mar 2021 22:18:37 -0700</pubDate><guid>https://lijohnny.com/p/price-strategy-based-on-features-reputation/</guid><description>Customer reviews contain a large amount of information. One of the recurring subjects of NLP is to understand customer opinion through statement analysis of customer reviews. However, Basic Sentimental Classification can only tell customers' overall impression about the product. It can&amp;rsquo;t tell customers' opinions of specific features of the product. But, if we dive deeper into the customer reviews, we can get more information. My goal is to extract features from reviews, identify each feature&amp;rsquo;s opinion, quantify the sentiment using econometrics, and then generate a price strategy based on it.</description></item><item><title>Identify Customer Segments</title><link>https://lijohnny.com/p/identify-customer-segments/</link><pubDate>Fri, 12 Mar 2021 21:42:22 -0800</pubDate><guid>https://lijohnny.com/p/identify-customer-segments/</guid><description>This is one of the Udacity Data Scientist Nanodegree Project. This project aims to identify segments of the population from the core customer base for a mail-order sales company in Germany. Therefore, these segments can then be used to direct marketing campaigns towards audiences with the highest expected rate of returns.
The techniques I used in this project include:
Data cleaning Encoding and processing mixed-type feature Feature Scaling and Dimensionality Reduction Clustering Performance improvement with OpenBLAS You can find the full analysis in my GitHub repo.</description></item><item><title>Donors Identification for CharityML</title><link>https://lijohnny.com/p/donors-identification-for-charityml/</link><pubDate>Thu, 30 May 2019 01:03:50 +0000</pubDate><guid>https://lijohnny.com/p/donors-identification-for-charityml/</guid><description>&lt;p>&lt;a class="link" href="https://lijohnny.com" target="_blank" rel="noopener"
>&lt;img src="https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg" alt="" />&lt;/a> &lt;a class="link" href="https://www.python.org/" target="_blank" rel="noopener"
>&lt;img src="https://img.shields.io/badge/Made%20with-Python-1f425f.svg" alt="" />&lt;/a> &lt;a class="link" href="" >&lt;img src="https://img.shields.io/badge/Kaggle-Project-blue.svg" alt="" />&lt;/a>&lt;/p>
&lt;p>In this project, I built machine learning models that best identifies potential donors for CharityML(a fictitious charity organization) with data collected for the U.S. census. To find the best approach, I performed EDA, feature engineering, and building training and predicting pipeline to evaluate and optimize the performance between different machine learning models.&lt;/p>
&lt;p>The modified census dataset consists of approximately 32,000 data points, with each datapoint having 13 features. This dataset is a modified version of the dataset published in the paper &lt;em>&amp;ldquo;Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid&amp;rdquo;,&lt;/em> by Ron Kohavi. You may find this paper &lt;a class="link" href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf" target="_blank" rel="noopener"
>online&lt;/a>, with the original dataset hosted on &lt;a class="link" href="https://archive.ics.uci.edu/ml/datasets/Census&amp;#43;Income" target="_blank" rel="noopener"
>UCI&lt;/a>.&lt;/p>
&lt;p>The model I have used:&lt;/p>
&lt;ul>
&lt;li>SGD Classifier&lt;/li>
&lt;li>AdaBoost&lt;/li>
&lt;li>Logistic Regression&lt;/li>
&lt;/ul></description></item><item><title>Automate Image Processing for My Blog</title><link>https://lijohnny.com/p/automate-image-processing-for-my-blog/</link><pubDate>Sat, 11 May 2019 22:28:30 +0000</pubDate><guid>https://lijohnny.com/p/automate-image-processing-for-my-blog/</guid><description>&lt;blockquote>
&lt;p>:warning:The old cript is &lt;code>DEPRECATED&lt;/code>. I switch to FastAPI, and updated the code &lt;a class="link" href="https://github.com/iamjohnnyli/blog-image-processing-automation" target="_blank" rel="noopener"
>here&lt;/a>. The old code is now in &lt;a class="link" href="https://github.com/iamjohnnyli/blog-image-processing-automation/tree/V1.0" target="_blank" rel="noopener"
>v1.0 Branch&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>As you can see, my blog contains cover images. Every time I write an article I need to upload a cover image to the image server. However, the process is complicated and trivial. I need to crop the image to a certain proportion and resize and compress the image to reduce the loading time. Moreover, to put the photographer&amp;rsquo;s name on the image, I need to use other software. Therefore, I create two python scripts to automate the process.&lt;/p></description></item><item><title>Spam Detection and Analysis (Part 2 - Analysis with R)</title><link>https://lijohnny.com/p/spam-detection-and-analysis-part-2-analysis-with-r/</link><pubDate>Thu, 09 May 2019 21:47:43 +0000</pubDate><guid>https://lijohnny.com/p/spam-detection-and-analysis-part-2-analysis-with-r/</guid><description>Spam is still a common attack method. Most of the email services have spam filters that can help us block and filter out most of the emails with commercial, fraudulent and malicious content. The purpose of the project is to explore and analyze the email header to identify the features that can tell us which emails are malicious.
This is the Part 2 of the series. For how to parsing email, please check Part 1</description></item><item><title>Spam Detection and Analysis (Part 1 - Parsing)</title><link>https://lijohnny.com/p/spam-detection-and-analysis-part-1-parsing/</link><pubDate>Fri, 19 Apr 2019 01:24:59 +0000</pubDate><guid>https://lijohnny.com/p/spam-detection-and-analysis-part-1-parsing/</guid><description>Spam is still a common attack method. Most of the email services have spam filters that can help us block and filter out most of the emails with commercial, fraudulent and malicious content. The purpose of the project is to explore and analyze the email header to identify the features that can tell us which emails are malicious.
Get Email Header information First thing we need to do is the get the data.</description></item><item><title>Solve the Problem in a April Fool's Joke</title><link>https://lijohnny.com/p/solve-the-problem-in-a-april-fools-joke/</link><pubDate>Sat, 06 Apr 2019 14:35:05 +0000</pubDate><guid>https://lijohnny.com/p/solve-the-problem-in-a-april-fools-joke/</guid><description>On April 1st, one of the post on v2ex (An online community of developers and designers in China1) become viral. The post title is &amp;ldquo;Seeking Dates&amp;rdquo; and it is posted on April 1st, so most reactions of this post are pointing out that this is the &amp;ldquo;new trap&amp;rdquo; of the recruiters.
Here is the translation:
I am born and raised in Hangzhou. I am a science student at Zhejiang University &amp;hellip; I hope to find a man can pass the following test.</description></item><item><title>Craigslist Vacation House Data Crawling</title><link>https://lijohnny.com/p/craigslist-vacation-house-data-crawling/</link><pubDate>Tue, 12 Feb 2019 01:13:34 -0800</pubDate><guid>https://lijohnny.com/p/craigslist-vacation-house-data-crawling/</guid><description>&lt;p>In this &lt;a class="link" href="https://github.com/iamjohnnyli/web-crawler-tutorial/tree/master/scrapy_craigslist" target="_blank" rel="noopener"
>tutorial&lt;/a> I use &lt;a class="link" href="https://scrapy.org/" target="_blank" rel="noopener"
>Scrapy&lt;/a> to collect data from Craigslist.com. Specifically, the data under craigslist.org/Seattle/housing/vacation rentals. You can find the page under the link: &lt;a class="link" href="https://seattle.craigslist.org/d/vacation%e2%80%90rentals/search/vac" target="_blank" rel="noopener"
>https://seattle.craigslist.org/d/vacation‐rentals/search/vac&lt;/a>&lt;/p></description></item></channel></rss>