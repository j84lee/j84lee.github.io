[{"content":"Disaster Response Web Application is a Web app that can help emergency organizations analyze incoming messages and classify the messages into specific categories (e.g. Water, Food, Hospitals, Aid-Related) during a disaster event. The app is based on Nature Language Processing and Random Forest Classifier ML model. The data was collected by Figure Eight and provided by Udacity.\nThe techniques I used in this project include:\n SQLite in Python Bag of Words (BOW) Multilabel classification Build Machine Learning Pipeline Grid Search Cross Validation Random Forest Classifier K nearest neighbors (KNN) Build a Flask web app   You can find the full analysis in my GitHub repo.\n Data The data files associated with this project are from Figure Eight\n messages.csv: FIgure Eight provide 26,248 messages categories.csv: Raw categories data, total 36 categories.  Project Process   ETL Pipeline\n Loads messages and categories dataset Clean data Feature Engineering Stores it in a SQLite database    ML Pipeline\n Loads data from the SQLite database text processing and machine learning pipeline Trains and tunes a model using GridSearchCV Exports the model    Build web app\n Create HTML templates Build Flask web application    Folder Structure ├── README.md │ ├── models \u0026lt;- Trained models and ML pipeline │ ├── classifier.pkl \u0026lt;- Saved model │ └── train_classifier.py \u0026lt;- Scripts to train model sdf │ ├── requirements.txt \u0026lt;- File for reproducing the environment │ ├── data \u0026lt;- Raw and processed Data; data │ │ cleaning script │ ├── messages.csv \u0026lt;- Raw messages data │ ├── categories.csv \u0026lt;- Raw categories data │ ├── DisasterResponse.db \u0026lt;- Saved processed data │ └── process_data.py \u0026lt;- Scripts to process data │ ├── notebooks \u0026lt;- Jupyter notebooks │ └── App \u0026lt;- Source code for use in this project. ├── templates \u0026lt;- Flask html templates └── run.py \u0026lt;- Scripts to create start Flask server. Full Instructions   Installation Install Python 3.5+ Run pip install -r requirements.txt\n  Prepare data\n Clone or download the repo Open terminal and navigate to the project folder Run python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db    Train model 5. Run python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl\n  Start Web app 6. Run cd app 7. Run python run.py 8. Open web browser and go to http://loclhost:3001 or http://0.0.0.0:3001\n  Screen Shot  \n \n  .resp-sharing-button__link, .resp-sharing-button__icon { display: inline-block } .resp-sharing-button__link { text-decoration: none; color: #fff; margin: 0.5em } .resp-sharing-button { border-radius: 5px; transition: 25ms ease-out; padding: 0.5em 0.75em; font-family: Helvetica Neue,Helvetica,Arial,sans-serif } .resp-sharing-button__icon svg { width: 1em; height: 1em; margin-right: 0.4em; vertical-align: top } .resp-sharing-button--small svg { margin: 0; vertical-align: middle } .resp-sharing-button__icon { stroke: #fff; fill: none } .resp-sharing-button__icon--solid, .resp-sharing-button__icon--solidcircle { fill: #fff; stroke: none } .resp-sharing-button--twitter { background-color: #55acee } .resp-sharing-button--twitter:hover { background-color: #2795e9 } .resp-sharing-button--pinterest { background-color: #bd081c } .resp-sharing-button--pinterest:hover { background-color: #8c0615 } .resp-sharing-button--facebook { background-color: #3b5998 } .resp-sharing-button--facebook:hover { background-color: #2d4373 } .resp-sharing-button--tumblr { background-color: #35465C } .resp-sharing-button--tumblr:hover { background-color: #222d3c } .resp-sharing-button--reddit { background-color: #5f99cf } .resp-sharing-button--reddit:hover { background-color: #3a80c1 } .resp-sharing-button--google { background-color: #dd4b39 } .resp-sharing-button--google:hover { background-color: #c23321 } .resp-sharing-button--linkedin { background-color: #0077b5 } .resp-sharing-button--linkedin:hover { background-color: #046293 } .resp-sharing-button--email { background-color: #777 } .resp-sharing-button--email:hover { background-color: #5e5e5e } .resp-sharing-button--xing { background-color: #1a7576 } .resp-sharing-button--xing:hover { background-color: #114c4c } .resp-sharing-button--whatsapp { background-color: #25D366 } .resp-sharing-button--whatsapp:hover { background-color: #1da851 } .resp-sharing-button--hackernews { background-color: #FF6600 } .resp-sharing-button--hackernews:hover, .resp-sharing-button--hackernews:focus { background-color: #FB6200 } .resp-sharing-button--vk { background-color: #507299 } .resp-sharing-button--vk:hover { background-color: #43648c } .resp-sharing-button--facebook { background-color: #3b5998; border-color: #3b5998; } .resp-sharing-button--facebook:hover, .resp-sharing-button--facebook:active { background-color: #2d4373; border-color: #2d4373; } .resp-sharing-button--twitter { background-color: #55acee; border-color: #55acee; } .resp-sharing-button--twitter:hover, .resp-sharing-button--twitter:active { background-color: #2795e9; border-color: #2795e9; } .resp-sharing-button--email { background-color: #777777; border-color: #777777; } .resp-sharing-button--email:hover, .resp-sharing-button--email:active { background-color: #5e5e5e; border-color: #5e5e5e; } .resp-sharing-button--linkedin { background-color: #0077b5; border-color: #0077b5; } .resp-sharing-button--linkedin:hover, .resp-sharing-button--linkedin:active { background-color: #046293; border-color: #046293; } .resp-sharing-button--reddit { background-color: #5f99cf; border-color: #5f99cf; } .resp-sharing-button--reddit:hover, .resp-sharing-button--reddit:active { background-color: #3a80c1; border-color: #3a80c1; } .resp-sharing-button--telegram { background-color: #54A9EB; } .resp-sharing-button--telegram:hover { background-color: #4B97D1;} .share { margin: 1.5em 0; padding: 0 var(--card-padding); font-size: 0.8em; } .share svg{ margin-top: 0.3em; } .share a:hover{ color:#dddddd; } .share p{ font-size: 1.3em; margin:0 auto; }  If you think your friends/network would find this useful, please share it with them – I’d really appreciate it.\nFacebook  Twitter  E-Mail  LinkedIn  Reddit  Telegram    ","date":"2021-05-02T21:24:32-07:00","image":"https://lijohnny.com/p/disaster-response-web-app/ArpiP6q2msCtGnU_hu457b68f1b299c99b4ae6562f4e0f55f0_178477_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/disaster-response-web-app/","title":"Disaster Response Web App"},{"content":"Usually, the first step we all do for any Python project is importing required packages. So, the beginning of the file usually looks like this:\nimport xxxx as xxx from xxxx import xxxx Importing packages is one of my least favorite parts when I\u0026rsquo;m working on a Data Scientist project. Packages like pandas, numpy, lighteda, matplotlib.pyplot, and seaborn are commonly used for my projects. I need to repeat this step in every single Jupyter Notebook. Moreover, when the analysis/research dives deeper, and I need to import new packages in the middle, I have to go back to the beginning and import new packages (If you are using Jupyter Notebook, don\u0026rsquo;t forget to press Ctrl + Enter).\nGoing back and forth to import packages is frustrating.\nI can reduce some repetitive works by using template tools like cookiecutter.\n   But, is there a way to automatically import related packages and add import statements for me, like this? The packages are automatically imported in the first cell.\nBut how? Pyforest - a package can automatically import packages for you.   \nPyforest can \u0026ldquo;lazy\u0026rdquo; import the packages and add the import statement to your Jupyter Notebook. It will not import packages you are not using.\nYou can install it using pip:\npip install --upgrade pyforest python -m pyforest install_extensions The installation will add pyforest_autoimport.py to your Jupyter and IPython default startup settings. Therefore, you can start coding in Jupyter or Ipython and don\u0026rsquo;t need to import pyforest.\n I didn’t add the statement for import pandas as pd. \nAlso, you can notice that I didn\u0026rsquo;t type the full name of pandas library. Pyforest has already included all the common abbreviations.  Pyforest includes many import statements. \nYou can also edit ~/.pyforest/user_imports.py to add your customized import statements.\n \nPyforest works better on Jupyter Notebook or IPython. If you want this feature to work in other editors, you can import Pyforest first. e.g.\nimport pyforest pyforest.active_imports() This is my daily dose of Python tricks. I hope this post can help you. I strongly recommend you try cookiecutter and Pyforest to automate your Data Science/Analysis workflow.\n  .resp-sharing-button__link, .resp-sharing-button__icon { display: inline-block } .resp-sharing-button__link { text-decoration: none; color: #fff; margin: 0.5em } .resp-sharing-button { border-radius: 5px; transition: 25ms ease-out; padding: 0.5em 0.75em; font-family: Helvetica Neue,Helvetica,Arial,sans-serif } .resp-sharing-button__icon svg { width: 1em; height: 1em; margin-right: 0.4em; vertical-align: top } .resp-sharing-button--small svg { margin: 0; vertical-align: middle } .resp-sharing-button__icon { stroke: #fff; fill: none } .resp-sharing-button__icon--solid, .resp-sharing-button__icon--solidcircle { fill: #fff; stroke: none } .resp-sharing-button--twitter { background-color: #55acee } .resp-sharing-button--twitter:hover { background-color: #2795e9 } .resp-sharing-button--pinterest { background-color: #bd081c } .resp-sharing-button--pinterest:hover { background-color: #8c0615 } .resp-sharing-button--facebook { background-color: #3b5998 } .resp-sharing-button--facebook:hover { background-color: #2d4373 } .resp-sharing-button--tumblr { background-color: #35465C } .resp-sharing-button--tumblr:hover { background-color: #222d3c } .resp-sharing-button--reddit { background-color: #5f99cf } .resp-sharing-button--reddit:hover { background-color: #3a80c1 } .resp-sharing-button--google { background-color: #dd4b39 } .resp-sharing-button--google:hover { background-color: #c23321 } .resp-sharing-button--linkedin { background-color: #0077b5 } .resp-sharing-button--linkedin:hover { background-color: #046293 } .resp-sharing-button--email { background-color: #777 } .resp-sharing-button--email:hover { background-color: #5e5e5e } .resp-sharing-button--xing { background-color: #1a7576 } .resp-sharing-button--xing:hover { background-color: #114c4c } .resp-sharing-button--whatsapp { background-color: #25D366 } .resp-sharing-button--whatsapp:hover { background-color: #1da851 } .resp-sharing-button--hackernews { background-color: #FF6600 } .resp-sharing-button--hackernews:hover, .resp-sharing-button--hackernews:focus { background-color: #FB6200 } .resp-sharing-button--vk { background-color: #507299 } .resp-sharing-button--vk:hover { background-color: #43648c } .resp-sharing-button--facebook { background-color: #3b5998; border-color: #3b5998; } .resp-sharing-button--facebook:hover, .resp-sharing-button--facebook:active { background-color: #2d4373; border-color: #2d4373; } .resp-sharing-button--twitter { background-color: #55acee; border-color: #55acee; } .resp-sharing-button--twitter:hover, .resp-sharing-button--twitter:active { background-color: #2795e9; border-color: #2795e9; } .resp-sharing-button--email { background-color: #777777; border-color: #777777; } .resp-sharing-button--email:hover, .resp-sharing-button--email:active { background-color: #5e5e5e; border-color: #5e5e5e; } .resp-sharing-button--linkedin { background-color: #0077b5; border-color: #0077b5; } .resp-sharing-button--linkedin:hover, .resp-sharing-button--linkedin:active { background-color: #046293; border-color: #046293; } .resp-sharing-button--reddit { background-color: #5f99cf; border-color: #5f99cf; } .resp-sharing-button--reddit:hover, .resp-sharing-button--reddit:active { background-color: #3a80c1; border-color: #3a80c1; } .resp-sharing-button--telegram { background-color: #54A9EB; } .resp-sharing-button--telegram:hover { background-color: #4B97D1;} .share { margin: 1.5em 0; padding: 0 var(--card-padding); font-size: 0.8em; } .share svg{ margin-top: 0.3em; } .share a:hover{ color:#dddddd; } .share p{ font-size: 1.3em; margin:0 auto; }  If you think your friends/network would find this useful, please share it with them – I’d really appreciate it.\nFacebook  Twitter  E-Mail  LinkedIn  Reddit  Telegram    ","date":"2021-03-19T19:56:44-07:00","image":"https://lijohnny.com/p/pyforest-automate-package-import-process-for-your-data-science-project/j4dzCfs1YINnUeP_hu2e33a0807f480dffb7d29605e6b641f0_179685_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/pyforest-automate-package-import-process-for-your-data-science-project/","title":"Pyforest - Automate Package Import Process for Your Data Science Project"},{"content":"  \nCustomer reviews contain a large amount of information. One of the recurring subjects of NLP is to understand customer opinion through statement analysis of customer reviews. However, Basic Sentimental Classification can only tell customers' overall impression about the product. It can\u0026rsquo;t tell customers' opinions of specific features of the product. But, if we dive deeper into the customer reviews, we can get more information. My goal is to extract features from reviews, identify each feature\u0026rsquo;s opinion, quantify the sentiment using econometrics, and then generate a price strategy based on it.\nThe main techniques I used are:\n Web Clawing: Get customer reviews from Amazon. Nature Language Processing: Identify product features. Sentiment Analysis: Identify polarity of opinions regarding product features. Linear Regression: Regression analysis for opinions of the features and the price.   The Jupyter Notebook goes along with this post will be available in my Repo on my Github soon.\n Data Overview The product category I was working on is the home security camera. I scraped reviews of targe product and its competitors from Amazon.com. The dataset contains 53,657 reviews of 15 different home security cameras from Amazon.com with the following information for each one:\n   Name Description     Product Name of the Product   Username User\u0026rsquo;s ID   Stars 1-5 rating for the product   Title The subject of the review   Date The review date   Review Review text   Helpfulcount How many people click \u0026ldquo;Helpful\u0026rdquo; button   ReviewTotal How many comments under this review    The average length of reviews of the product is around 200 words, and most of the product has 2000 reviews.    \nMost of reviews are in the range of 4 years.  \nMethodology My approach includes 9 steps.\ngraph TD A( Amazon Review Raw Data ) -- B( Data Cleaning ) B -- C( Sentences Separation ) C -- D( Subject/Object Separation ) D -- E( Sentiment Scoring ) D -- F( Feature Extraction ) E -- G( Feature Reputation ) F -- G( Feature Reputation ) G -- H( Sentiment Quantifying ) H -- I( Price Premiums Prediction )  Conclusion  \nTo Be Continued.\n","date":"2021-03-14T22:18:37-07:00","image":"https://lijohnny.com/p/price-strategy-based-on-features-reputation/4BxDCOzgPulU89Y_hue3bb56a18a536ed35c90b3582bf24783_132999_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/price-strategy-based-on-features-reputation/","title":"Price Strategy Based on Features Reputation"},{"content":"This is one of the Udacity Data Scientist Nanodegree Project. This project aims to use unsupervised learning techniques to identify segments of the population from the core customer base for a mail-order sales company in Germany. Therefore, these segments can then be used to direct marketing campaigns towards audiences with the highest expected rate of returns.\nThe techniques I used in this project include:\n Data cleaning Encoding and processing mixed-type feature Feature Scaling and Dimensionality Reduction Clustering Performance improvement with OpenBLAS   You can find the full analysis in my GitHub repo.\n Data The data files associated with this project (not included in this repository):\n Udacity_AZDIAS_Subset.csv: Demographics data for the general population of Germany; 891,211 persons (rows) x 85 features (columns). Udacity_CUSTOMERS_Subset.csv: Demographics data for customers of a mail-order company; 191,652 persons (rows) x 85 features (columns). Data_Dictionary.md: Detailed information file about the features in the provided datasets. AZDIAS_Feature_Summary.csv: Summary of feature attributes for demographics data; 85 features (rows) x 4 columns  Analysis Structure  Data exploration and data cleaning (85% of the analysis) Feature Engineering (One Hot, Scaling, and PCA) Clustering with k-means  Conclusion  Figure 1. Proportions per cluster for general vs customer. \nI use the elbow method to find that 6 is the optimal number for clustering, which means the model segments customers and the general population into 61 groups. We can find the proportion of customers in cluster 2 is higher than the general population, which suggests people in cluster 2 are the target audience. We also find that, in cluster 3, the customer is underrepresented, which means people in that group are outside of the target demographics.\n Figure 2. Major differences between Cluster 2 and Cluster 3 \nComparing 2 segments, we can find there are some key differences. For example:\n Distance from building to point of sale: People in cluster 3 are closer to Pos then people in cluster 2. Wealth / Life Stage Typology: More people in cluster 2 are upper class Type of Building: Most builds in cluster 2 are residential build. Social status: Most people in cluster 2 are top earners and most people in cluster 3 are house owners.   conda create -y -n p38openblas -c conda-forge \u0026quot;python=3.8\u0026quot; scipy \u0026quot;blas=*=*openblas\u0026quot; Licensing and Acknowledgements Udacity Data Scientist provided the starting code for this project. Udacity partners at Bertelsmann Arvato Analytics provided the data.\n  Cluster -1 is the group I added for checking the proportion of data that miss more than 30% information. \u0026#x21a9;\u0026#xfe0e;\n   ","date":"2021-03-12T21:42:22-08:00","image":"https://lijohnny.com/p/identify-customer-segments/7dfc3OoaYZAnejX_hu6422aa484a4a450a6cb8ea5e52a35a00_155749_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/identify-customer-segments/","title":"Identify Customer Segments"},{"content":"It\u0026rsquo;s been a year since last time I update my blog. Recently, I want to get back to writing blog again. However, I encounter some issues.\n  Hexo, the static web generator I used, was not compatible with current node.js version in my system.\n  Somehow node.js was extremely slow in WSL2.\n  I prefer working with the language that I\u0026rsquo;m more familiar with.\n  Therefore, I start looking for other solutions. After comparing and trying several static web generators. I picked Hugo. The reason is sample. Hugo is fast, and I find the theme I like.\nHere is the screenshot for my perverse website. Hexo has tens of theme too. But I didn\u0026rsquo;t find one fit me needs. So, I created my own theme. The design is based on Material Kit.\n \n","date":"2021-03-03T20:03:50Z","image":"https://lijohnny.com/p/switch-from-hexo-to-hugo/bg2_hu5da266bf266ff9c4ec0e08fc6268ff92_1250157_120x120_fill_box_smart1_2.png","permalink":"https://lijohnny.com/p/switch-from-hexo-to-hugo/","title":"Switch from Hexo to Hugo"},{"content":"  \nIn this project, I built machine learning models that best identifies potential donors for CharityML(a fictitious charity organization) with data collected for the U.S. census. To find the best approach, I performed EDA, feature engineering, and building training and predicting pipeline to evaluate and optimize the performance between different machine learning models.\nThe modified census dataset consists of approximately 32,000 data points, with each datapoint having 13 features. This dataset is a modified version of the dataset published in the paper \u0026ldquo;Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\u0026rdquo;, by Ron Kohavi. You may find this paper online, with the original dataset hosted on UCI.\nThe model I have used:\n SGD Classifier AdaBoost Logistic Regression  You can see the code(iPython notebook) here.\n","date":"2019-05-30T01:03:50Z","image":"https://lijohnny.com/p/donors-identification-for-charityml/5cef9192b167778174_hua2b97d798d80bf04515e8d7c3343e69b_64782_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/donors-identification-for-charityml/","title":"Donors Identification for CharityML"},{"content":" :warning:The old cript is DEPRECATED. I switch to FastAPI, and updated the code here. The old code is now in v1.0 Branch.\n As you can see, my blog contains cover images. Every time I write an article I need to upload a cover image to the image server. However, the process is complicated and trivial. I need to crop the image to a certain proportion and resize and compress the image to reduce the loading time. Moreover, to put the photographer\u0026rsquo;s name on the image, I need to use other software. Therefore, I create two python scripts to automate the process.\n For more information and code script, please go to my GitHub page.\n These scripts are for automating the process of cropping, resizing, adding author\u0026rsquo;s information, compressing, and uploading an image.\n The compressing service I use tinypng The storage for image I user sm.ms This script works only for the image download from unsplash.com  Usage To use correctly, you need to create an api.txt file that contains only the API key for tinypng. To generate an API key, you can check https://tinypng.com/developers.\nmain.py can be easily run in the terminal with the command:\npython main.py Once it launched, you can drag and drop any image on it.\nautomator.py can be used in Mac\u0026rsquo;s Automator. By building a server with Automator, I can right click on any image and run the script, or use a shortcut to run the script.\n","date":"2019-05-11T22:28:30Z","image":"https://lijohnny.com/p/automate-image-processing-for-my-blog/5cd7b069e734a_hu7215694eb30026f581cb7820db2005c7_69790_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/automate-image-processing-for-my-blog/","title":"Automate Image Processing for My Blog"},{"content":"Spam is still a common attack method. Most of the email services have spam filters that can help us block and filter out most of the emails with commercial, fraudulent and malicious content. The purpose of the project is to explore and analyze the email header to identify the features that can tell us which emails are malicious.\n This is the Part 2 of the series. For how to parsing email, please check Part 1\n Data Exploration Import the data to R For This part of the script, I used R language to explore the data and the build model. Therefore We need to import CSV to R first.\nem \u0026lt;- read.csv(\u0026#34;gamilspamedited.csv\u0026#34;,stringsAsFactors = F) 'data.frame':\t272 obs. of 18 variables: $ Subject : chr $ Return.Path.Address: chr $ Date : chr $ Reply.To.Address : chr $ Content.Type : chr $ From.Address : chr $ IP : chr $ SPF : chr $ DMARC : chr $ DKIM : chr $ Country : chr $ Regin : chr $ City : chr $ IPv6.Indicator : int $ CAT : chr $ Reputation : chr  After cleaning the data, there are 272 entries and 18 columns left.\nPreprocess the date Create Variables Create hashtml and hasattach columns to label if the email use html for main content and if the email has attachment.\nlibrary(dplyr) hashtml.func \u0026lt;- function(x,c1){ x \u0026lt;- case_when(x[c1]==\u0026#34;multipart/alternative\u0026#34; ~ \u0026#34;1\u0026#34;,x[c1]==\u0026#34;multipart/mixed\u0026#34; ~ \u0026#34;1\u0026#34;,x[c1]==\u0026#34;multipart/related\u0026#34; ~ \u0026#34;1\u0026#34;,x[c1]==\u0026#34;text/html\u0026#34; ~ \u0026#34;1\u0026#34;,x[c1]==\u0026#34;text/plain\u0026#34; ~ \u0026#34;0\u0026#34;) } em$hashtml \u0026lt;- apply(em,1,hashtml.func,c1=\u0026#39;Content.Type\u0026#39;) hasattach.func \u0026lt;- function(x,c1){ x \u0026lt;- case_when(x[c1]==\u0026#34;multipart/alternative\u0026#34; ~ \u0026#34;0\u0026#34;,x[c1]==\u0026#34;multipart/mixed\u0026#34; ~ \u0026#34;1\u0026#34;,x[c1]==\u0026#34;multipart/related\u0026#34; ~ \u0026#34;0\u0026#34;,x[c1]==\u0026#34;text/html\u0026#34; ~ \u0026#34;0\u0026#34;,x[c1]==\u0026#34;text/plain\u0026#34; ~ \u0026#34;0\u0026#34;) } em$hasattach \u0026lt;- apply(em,1,hasattach.func,c1=\u0026#39;Content.Type\u0026#39;) Compare Email\u0026rsquo;s From address and reply address and label if they are different.\nlibrary(stringr) rpavsfrom.func \u0026lt;- function(x){ fa\u0026lt;-x[\u0026#39;From.Address\u0026#39;] fa\u0026lt;-str_split(fa,\u0026#39;@\u0026#39;)[[1]][2] fasplit \u0026lt;-str_split(fa,\u0026#39;[.]\u0026#39;)[[1]] fareal \u0026lt;- paste(str_split(fa,\u0026#39;[.]\u0026#39;)[[1]][(length(fasplit)-1)],\u0026#39;.\u0026#39;,str_split(fa,\u0026#39;[.]\u0026#39;)[[1]][(length(fasplit))],sep = \u0026#34;\u0026#34;) rp\u0026lt;-x[\u0026#39;Return.Path.Address\u0026#39;] rp\u0026lt;-str_split(rp,\u0026#39;@\u0026#39;)[[1]][2] rpsplit \u0026lt;-str_split(rp,\u0026#39;[.]\u0026#39;)[[1]] rpreal \u0026lt;- paste(str_split(rp,\u0026#39;[.]\u0026#39;)[[1]][(length(rpsplit)-1)],\u0026#39;.\u0026#39;,str_split(rp,\u0026#39;[.]\u0026#39;)[[1]][(length(rpsplit))],sep = \u0026#34;\u0026#34;) if (fareal == rpreal) { x = 1 }else{ x = 0 } return(x) } em$rpavsfrom\u0026lt;-apply(em,1,rpavsfrom.func) library(stringr) replyvsfrom.func \u0026lt;- function(x){ fa\u0026lt;-x[\u0026#39;From.Address\u0026#39;] fa\u0026lt;-str_split(fa,\u0026#39;@\u0026#39;)[[1]][2] fasplit \u0026lt;-str_split(fa,\u0026#39;[.]\u0026#39;)[[1]] fareal \u0026lt;- paste(str_split(fa,\u0026#39;[.]\u0026#39;)[[1]][(length(fasplit)-1)],\u0026#39;.\u0026#39;,str_split(fa,\u0026#39;[.]\u0026#39;)[[1]][(length(fasplit))],sep = \u0026#34;\u0026#34;) ra\u0026lt;-x[\u0026#39;Reply.To.Address\u0026#39;] ra\u0026lt;-str_split(ra,\u0026#39;@\u0026#39;)[[1]][2] rasplit \u0026lt;-str_split(ra,\u0026#39;[.]\u0026#39;)[[1]] rareal \u0026lt;- paste(str_split(ra,\u0026#39;[.]\u0026#39;)[[1]][(length(rasplit)-1)],\u0026#39;.\u0026#39;,str_split(ra,\u0026#39;[.]\u0026#39;)[[1]][(length(rasplit))],sep = \u0026#34;\u0026#34;) if (fareal == rareal) { x = 1 }else{ x = 0 } return(x) } em$replyvsfrom\u0026lt;-apply(em,1,replyvsfrom.func) Bin and Categorize the Time and the Day.\nlibrary(lubridate) weekday.func \u0026lt;- function(x,c1){ weekday \u0026lt;- wday(as.Date(str_split(x[c1],\u0026#39; \u0026#39;)[[1]][1],format=\u0026#39;%m/%d/%Y\u0026#39;)) return(weekday) } em$Weekday \u0026lt;- apply(em,1,weekday.func,c1=\u0026#39;Date\u0026#39;) em$Weekday \u0026lt;- factor(em$Weekday) levels(em$Weekday) \u0026lt;- c(\u0026#34;Mon\u0026#34;, \u0026#34;Tue\u0026#34;, \u0026#34;Wed\u0026#34;, \u0026#34;Thu\u0026#34;, \u0026#34;Fri\u0026#34;, \u0026#34;Sat\u0026#34;, \u0026#34;Sun\u0026#34;) time.func \u0026lt;- function(x,c1){ time \u0026lt;- gsub(\u0026#34;:\u0026#34;, \u0026#34;\u0026#34;, str_split(x[c1],\u0026#39; \u0026#39;)[[1]][2]) return(time) } em$Time \u0026lt;- apply(em,1,time.func,c1=\u0026#39;Date\u0026#39;) em$Time \u0026lt;- factor(round(as.numeric(em$Time)/100)) Convert the data to proper formats.\nem$SPF \u0026lt;- factor(em$SPF,levels = c(\u0026#34;pass\u0026#34;,\u0026#34;neutral\u0026#34;,\u0026#34;fail\u0026#34;,\u0026#34;temperror\u0026#34;)) em$DMARC \u0026lt;- factor(em$DMARC,levels = c(\u0026#34;pass\u0026#34;,\u0026#34;fail\u0026#34;,\u0026#34;None\u0026#34;)) em$DKIM \u0026lt;- factor(em$DKIM,levels = c(\u0026#34;pass\u0026#34;,\u0026#34;None\u0026#34;)) em$Country \u0026lt;- factor(em$Country) em$IPv6.Indicator\u0026lt;-factor(em$IPv6.Indicator) em$CAT \u0026lt;- factor(em$CAT) em$Reputation \u0026lt;- factor(em$Reputation) em$hashtml \u0026lt;- factor(em$hashtml) em$hasattach \u0026lt;- factor(em$hasattach) em$rpavsfrom \u0026lt;- factor(em$rpavsfrom) em$replyvsfrom \u0026lt;- factor(em$replyvsfrom) Set base level.\nem$Country \u0026lt;- relevel(em$Country,ref = \u0026#34;US\u0026#34;) em$Reputation \u0026lt;- relevel(em$Reputation,ref = \u0026#34;Unsuspicious\u0026#34;) Re-code the target to numeric.\nem$CAT \u0026lt;- as.numeric(em$CAT == \u0026#34;Spam\u0026#34;) Explore the Relationship.\nset.seed(11) selected.var \u0026lt;- c(7, 9, 10, 11, 12, 15, 16,17,18,19,20,21,22,23,24) selected.df \u0026lt;- em[, selected.var] emnum\u0026lt;-selected.df levels(selected.df$SPF) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;) emnum$SPF \u0026lt;- as.numeric(selected.df$SPF) levels(selected.df$DMARC) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;) emnum$DMARC \u0026lt;- as.numeric(selected.df$DMARC) levels(selected.df$DKIM) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$DKIM \u0026lt;- as.numeric(selected.df$DKIM) levels(selected.df$Country) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$Country \u0026lt;- as.numeric(selected.df$Country) levels(selected.df$IPv6.Indicator) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$IPv6.Indicator \u0026lt;- as.numeric(selected.df$IPv6.Indicator) levels(selected.df$Reputation) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$Reputation \u0026lt;- as.numeric(selected.df$Reputation) levels(selected.df$hashtml) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$hashtml \u0026lt;- as.numeric(selected.df$hashtml) levels(selected.df$hasattach) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$hasattach \u0026lt;- as.numeric(selected.df$hasattach) levels(selected.df$rpavsfrom) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$rpavsfrom \u0026lt;- as.numeric(selected.df$rpavsfrom) levels(selected.df$replyvsfrom) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) emnum$replyvsfrom \u0026lt;- as.numeric(selected.df$replyvsfrom) levels(selected.df$Weekday) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;5\u0026#34;, \u0026#34;6\u0026#34;, \u0026#34;7\u0026#34;) emnum$Weekday \u0026lt;- as.numeric(selected.df$Weekday) levels(selected.df$Time) \u0026lt;- c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;5\u0026#34;, \u0026#34;6\u0026#34;, \u0026#34;7\u0026#34;, \u0026#34;8\u0026#34;, \u0026#34;9\u0026#34;, \u0026#34;10\u0026#34;, \u0026#34;11\u0026#34;, \u0026#34;12\u0026#34;, \u0026#34;13\u0026#34;, \u0026#34;14\u0026#34;,\u0026#34;15\u0026#34;, \u0026#34;16\u0026#34;, \u0026#34;17\u0026#34;, \u0026#34;18\u0026#34;, \u0026#34;19\u0026#34;, \u0026#34;20\u0026#34;, \u0026#34;21\u0026#34;, \u0026#34;22\u0026#34;, \u0026#34;23\u0026#34;, \u0026#34;24\u0026#34;, \u0026#34;25\u0026#34;) emnum$Time \u0026lt;- as.numeric(selected.df$Time) Create correlation matrix.\nlibrary(\u0026#34;PerformanceAnalytics\u0026#34;) chart.Correlation(emnum, method=\u0026#34;spearman\u0026#34;,histogram=TRUE,pch=\u0026#34;16\u0026#34;) library(corrr) emnum %\u0026gt;% cor(method=\u0026#34;spearman\u0026#34;) %\u0026gt;% network_plot(min_cor=0.1) Build Model Partition Data Create Training and Validation Sets\n# partition the data train.index \u0026lt;- sample(1:nrow(em), nrow(em)*0.6) train.df \u0026lt;- selected.df[train.index, ] valid.df \u0026lt;- selected.df[-train.index, ] Fit a Logistic Regression Model\nlogit.reg \u0026lt;- glm(CAT ~ Message+DMARC+Country+DKIM+Reputation+replyvsfrom+rpavsfrom+Weekday, data = train.df, family = \u0026#34;binomial\u0026#34;) summary(logit.reg) Call: glm(formula = CAT ~ Message + DMARC + Country + DKIM + Reputation + replyvsfrom + rpavsfrom + Weekday, family = \u0026quot;binomial\u0026quot;, data = train.df) Deviance Residuals: Min 1Q Median 3Q Max -3.09465 -0.18311 -0.04540 -0.00298 2.73259 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -4.880e+00 1.891e+00 -2.581 0.009848 ** Message -2.864e-05 1.803e-05 -1.589 0.112145 DMARC2 -1.101e+01 2.400e+03 -0.005 0.996340 DMARC3 6.306e+00 1.445e+00 4.364 1.28e-05 *** Country2 4.332e+00 1.767e+00 2.451 0.014247 * DKIM2 -3.987e+00 2.255e+00 -1.768 0.077077 . Reputation2 9.137e+00 2.607e+00 3.505 0.000457 *** replyvsfrom2 -5.052e+00 1.214e+00 -4.162 3.16e-05 *** rpavsfrom2 -2.141e+00 1.044e+00 -2.052 0.040209 * Weekday2 3.313e+00 1.764e+00 1.878 0.060359 . Weekday3 3.392e+00 1.544e+00 2.197 0.028011 * Weekday4 6.549e-01 1.684e+00 0.389 0.697268 Weekday5 5.326e+00 1.607e+00 3.315 0.000918 *** Weekday6 1.686e+00 1.216e+00 1.387 0.165409 Weekday7 2.898e+00 1.634e+00 1.773 0.076224 . --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 172.126 on 162 degrees of freedom Residual deviance: 56.569 on 148 degrees of freedom AIC: 86.569 Number of Fisher Scoring iterations: 15  Predict with validation dataset and find the best threshold.\nlogit.reg.pred \u0026lt;- predict(logit.reg, valid.df, type = \u0026#34;response\u0026#34;) library(pROC) r \u0026lt;- roc(valid.df$CAT, logit.reg.pred) plot.roc(r) pred \u0026lt;- ifelse(logit.reg.pred \u0026gt; coords(r, x = \u0026#34;best\u0026#34;)[[1]], 1, 0) print(coords(r, x = \u0026#34;best\u0026#34;)) Type 'citation(\u0026quot;pROC\u0026quot;)' for a citation. Attaching package: 'pROC' The following objects are masked from 'package:stats': cov, smooth, var threshold specificity sensitivity 0.3222076 0.9625000 0.9655172  library(caret) confusionMatrix(factor(pred), factor(valid.df$CAT), positive = \u0026#34;1\u0026#34;) Loading required package: lattice Loading required package: ggplot2 Confusion Matrix and Statistics Reference Prediction 0 1 0 77 1 1 3 28 Accuracy : 0.9633 95% CI : (0.9087, 0.9899) No Information Rate : 0.7339 P-Value [Acc \u0026gt; NIR] : 2.433e-10 Kappa : 0.9081 Mcnemar's Test P-Value : 0.6171 Sensitivity : 0.9655 Specificity : 0.9625 Pos Pred Value : 0.9032 Neg Pred Value : 0.9872 Prevalence : 0.2661 Detection Rate : 0.2569 Detection Prevalence : 0.2844 Balanced Accuracy : 0.9640 'Positive' Class : 1  ","date":"2019-05-09T21:47:43Z","image":"https://lijohnny.com/p/spam-detection-and-analysis-part-2-analysis-with-r/5cd5051a0e1f9_hu17c392432df8ca411e01c9374606a277_619113_120x120_fill_box_smart1_2.png","permalink":"https://lijohnny.com/p/spam-detection-and-analysis-part-2-analysis-with-r/","title":"Spam Detection and Analysis (Part 2 - Analysis with R)"},{"content":"Spam is still a common attack method. Most of the email services have spam filters that can help us block and filter out most of the emails with commercial, fraudulent and malicious content. The purpose of the project is to explore and analyze the email header to identify the features that can tell us which emails are malicious.\nGet Email Header information First thing we need to do is the get the data. Email Header contents a lots of information that usually a email client do not show that to the users. However, most of those informations reveal more information about the sender. Therefore, we need to get Email Headers and parsing them for analysis. I use Python\u0026rsquo;s imaplib library to get email headers from my Gmail account Spam folder.\n For the script to work, Gmail IMAP Access need to be enabled. To enable IMAP for Gmail, please check this instruction. You also need to either turn off 2-Step Verification for your Google account or set up an app password here.\n Setup connection  Import the required libraries.  import getpass import imaplib import email from email.parser import HeaderParser from email.header import decode_header import re import csv import pandas as pd from bs4 import BeautifulSoup import requests import json Login to the Gmail  print(\u0026#39;Email:\u0026#39;) un = input() print(\u0026#39;Password\u0026#39;) pw = getpass.getpass() # hide passowrad with getpass() function. conn = imaplib.IMAP4_SSL(port = \u0026#39;993\u0026#39;,host = \u0026#39;imap.gmail.com\u0026#39;) conn.login(un,pw) Download Email and Parsing Email Header  Get email list.  conn.select(\u0026#39;[Gmail]/Spam\u0026#39;) # Get email form spam folder. type, emaildata = conn.search(None, \u0026#39;ALL\u0026#39;) emaillist=emaildata[0].split() # Get email list. parser = HeaderParser() header_list=[] msg_text=[] # Read API key for ipstack.com from key.txt file. key_f=open(\u0026#34;key.txt\u0026#34;,\u0026#34;r\u0026#34;) key=key_f.readlines()[0] Parsing email  Usually an email header looks like this: What we want are:\n   Name Description     Subject The Subject of Email   Reply-To The reply address that when you click reply button in you Email client   Return-Path The reply address that when you click reply button in you Email client   IP The sender\u0026rsquo;s IP address   Received Time When is the email received   From The senders\u0026rsquo;s address   Date The date the message was sent   Content-Type Indicated what contents in the email   Message The email content   DMARC Domain-Based Message Authentication Reporting and Conformance   DKIM Domain Keys Identified Mail   ARC-Authentication-Results Authenticated Received Chain   SPF Sender Policy Framework    Now we need to parse each email in the emaillist by using for loop.\nfor a in emaillist: type, emaildata2 = conn.fetch(a, \u0026#39;(RFC822)\u0026#39;) h = parser.parsestr(emaildata2[0][1].decode(\u0026#39;utf-8\u0026#39;,\u0026#39;ignore\u0026#39;)) Some emails have an html format message. The html tags are not helping us in this project. Therefore, we can use BeautifulSoup to get text from the email message.\nfor txt in h.walk(): if not txt.is_multipart(): msg_text = txt.get_payload(decode=True).decode(\u0026#39;utf-8\u0026#39;,\u0026#39;ignore\u0026#39;) soup = BeautifulSoup(msg_text, \u0026#34;lxml\u0026#34;) msg_text= soup.get_text(strip=True) Most information is clear and do not need clean and edit.\nheader = {} header[\u0026#39;Subject\u0026#39;]=decode_header(h[\u0026#39;Subject\u0026#39;])[0][0] header[\u0026#39;ARC-Authentication-Results\u0026#39;]=h[\u0026#39;ARC-Authentication-Results\u0026#39;].strip() header[\u0026#39;Return-Path\u0026#39;]=h[\u0026#39;Return-Path\u0026#39;].strip() header[\u0026#39;Return-Path Address\u0026#39;]=re.findall(r\u0026#39;\\b@\\S*\\b\u0026#39;, str(h[\u0026#39;Return-Path\u0026#39;].strip()))[0] header[\u0026#39;Received\u0026#39;]=h[\u0026#39;Received\u0026#39;].strip() header[\u0026#39;Received-SPF\u0026#39;]=h[\u0026#39;Received-SPF\u0026#39;].strip() header[\u0026#39;Date\u0026#39;]=pd.to_datetime(h[\u0026#39;Date\u0026#39;].strip()) if \u0026#39;Reply-To\u0026#39; in h: header[\u0026#39;Reply-To\u0026#39;]=h[\u0026#39;Reply-To\u0026#39;].strip() header[\u0026#39;Reply-To Address\u0026#39;]=re.findall(r\u0026#39;\\b@\\S*\\b\u0026#39;, str(h[\u0026#39;Reply-To\u0026#39;].strip()))[0] else: header[\u0026#39;Reply-To\u0026#39;]=\u0026#39;\u0026#39; header[\u0026#39;Reply-To Address\u0026#39;]=\u0026#39;\u0026#39; header[\u0026#39;Content-Type\u0026#39;]=h[\u0026#39;Content-Type\u0026#39;].strip().split(\u0026#39;;\u0026#39;)[0] header[\u0026#39;From\u0026#39;]=h[\u0026#39;From\u0026#39;].strip() However, for the data contents many irrelevant information and need to be cleaned. I use Regular expression operations to extract only useful information.\nif re.findall(r\u0026#39;\\b@\\S*\\b\u0026#39;,str(h[\u0026#39;From\u0026#39;].strip())): header[\u0026#39;From Address\u0026#39;]=re.findall(r\u0026#39;\\b@\\S*\\b\u0026#39;,str(h[\u0026#39;From\u0026#39;].strip()))[0] else: header[\u0026#39;From Address\u0026#39;]=\u0026#39;\u0026#39; #header[\u0026#39;Sender-ip\u0026#39;]= re.findall(r\u0026#39;(?:(?:25[0-5]|2[0-4]\\d|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\u0026#39;,str(header)) header[\u0026#39;Message\u0026#39;]= len(msg_text) if re.findall(r\u0026#39;\\bip=\\S*\\b\u0026#39;,str(header)): header[\u0026#39;IP\u0026#39;]=re.findall(r\u0026#39;\\bip=\\S*\\b\u0026#39;,str(header))[0].split(\u0026#39;=\u0026#39;)[1] else: header[\u0026#39;IP\u0026#39;]=\u0026#39;\u0026#39; if re.findall(r\u0026#39;\\bspf=\\S*\\b\u0026#39;,str(header)): header[\u0026#39;SPF\u0026#39;]=re.findall(r\u0026#39;\\bspf=\\S*\\b\u0026#39;,str(header))[0].split(\u0026#39;=\u0026#39;)[1] else: header[\u0026#39;SPF\u0026#39;]=\u0026#39;\u0026#39; if re.findall(r\u0026#39;\\bdmarc=\\S*\\b\u0026#39;,str(header)): header[\u0026#39;DMARC\u0026#39;]=re.findall(r\u0026#39;\\bdmarc=\\S*\\b\u0026#39;,str(header))[0].split(\u0026#39;=\u0026#39;)[1] else: header[\u0026#39;DMARC\u0026#39;]=\u0026#39;\u0026#39; if re.findall(r\u0026#39;\\bdkim=\\S*\\b\u0026#39;,str(header)): header[\u0026#39;DKIM\u0026#39;]=re.findall(r\u0026#39;\\bdkim=\\S*\\b\u0026#39;,str(header))[0].split(\u0026#39;=\u0026#39;)[1] else: header[\u0026#39;DKIM\u0026#39;]=\u0026#39;\u0026#39; I use the api of ipstack.com to convert IP to geographic location.\naddress = \u0026#34;http://api.ipstack.com/\u0026#34;+header[\u0026#39;IP\u0026#39;]+\u0026#34;?access_key=\u0026#34;+key response = requests.get(address) ipjason = response.text iplist = json.loads(ipjason) header[\u0026#39;Country\u0026#39;]= iplist.get(\u0026#39;country_name\u0026#39;) header[\u0026#39;Regin\u0026#39;]= iplist.get(\u0026#39;region_name\u0026#39;) header[\u0026#39;City\u0026#39;]= iplist.get(\u0026#39;city\u0026#39;) if iplist.get(\u0026#39;type\u0026#39;)== \u0026#39;ipv6\u0026#39; : header[\u0026#39;IPv6 Indicator\u0026#39;]= 1 elif iplist.get(\u0026#39;type\u0026#39;)== \u0026#39;ipv4\u0026#39; : header[\u0026#39;IPv6 Indicator\u0026#39;]= 0 header_list.append(header) # For Loop END For consistent analysis, I save the DataFrame into a CSV file for further analysis.\nkeys = header_list[0].keys() print(\u0026#39;File Name:\u0026#39;) fn = input() with open(fn+\u0026#39;.csv\u0026#39;, \u0026#39;w\u0026#39;) as output_file: dict_writer = csv.DictWriter(output_file, keys) dict_writer.writeheader() dict_writer.writerows(header_list)  This is the Part 1 of the series. For how to user R analysis the email, please check Part 2\n ","date":"2019-04-19T01:24:59Z","image":"https://lijohnny.com/p/spam-detection-and-analysis-part-1-parsing/5cd50420b955f_hu9b8175d3ba48e13699867e4777819883_306711_120x120_fill_box_smart1_2.png","permalink":"https://lijohnny.com/p/spam-detection-and-analysis-part-1-parsing/","title":"Spam Detection and Analysis (Part 1 - Parsing)"},{"content":"On April 1st, one of the post on v2ex (An online community of developers and designers in China1) become viral. The post title is \u0026ldquo;Seeking Dates\u0026rdquo; and it is posted on April 1st, so most reactions of this post are pointing out that this is the \u0026ldquo;new trap\u0026rdquo; of the recruiters.\nHere is the translation:\n I am born and raised in Hangzhou. I am a science student at Zhejiang University \u0026hellip; I hope to find a man can pass the following test. My Wechat ID is my Family name Lin followed with two prime number. The smaller prime number is in the front. The product of these two prime numbers 7140229933. If you can find my Wechat ID, I will sincerely date you. Bula bula bula \u0026hellip;\n Disregard if it is a joke, I think the question is interesting. So let\u0026rsquo;s solve it.\nThe idea is using Exhaustive Method (Brute Force Method) to find two prime factors. However, 7,140,229,933 is a relatively large number. To find all prime number could take a long time. Since 7,140,229,933 is the product of two prime, one of the numbers must smaller than the Square Root of it. Therefore the order of magnitude of range could reduce from $10^9$ to $10^4$. Here is my solution.\nimport time start = time.time() # The input number input = 7140229933 for x in range (2,int(input**(0.5))): if input%x ==0: print(x) break print(int(input / 83777)) end = time.time() print(\u0026#39;Total Time: {}s.\u0026#39;.format(end - start)) Output:\n83777 85229 Total Time: 0.01737499237060547s. So the \u0008two prime number are 83777 and 85229.\n  https://zh.wikipedia.org/wiki/V2EX \u0026#x21a9;\u0026#xfe0e;\n   ","date":"2019-04-06T14:35:05Z","image":"https://lijohnny.com/p/solve-the-problem-in-a-april-fools-joke/5cc38cc539c01_hue631fbcea079f93726f457762c59725d_381683_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/solve-the-problem-in-a-april-fools-joke/","title":"Solve the Problem in a April Fool's Joke"},{"content":"Data Analysis is not all about reports or visualization. The correctness and reproducibility are also important for scientific research. A consistent environment is critical for reproducibility. There are several ways to achieve that. However, I find out using Docker at any time can repeat the experiment in the same environment. It is easy to scale up and scale horizontally.\nMy docker image is based on Ubuntu. It includes common Data Science tools such as Jupyter Notebook wiht Python 3 and R kernel. With the help of R Magic, I can run both Python and R in the same .ipynb file. To learn more about R Magic, you can click here. I also installed Nbextensions for Jupyter Notebook. For more information, you can click here\n You can find my Dockerfile in my GitHub Repository.\n HOW TO USE MY DOCKERFILE Install Docker The Docker community have an explicit tutorial about how to install Docker. Please check here\nBuild In the terminal, direct to the folder that contains the dockerfile and run the following command:\ndocker build -t data-analyst-notebook . Don\u0026rsquo;t forget the \u0026ldquo;.\u0026rdquo; at the end. data-analyst-notebook is the name of the image. You can change to whatever you prefer.\nStart server I use following code to start server:\ndocker run --rm -p 8888:8888 -e JUPYTER_ENABLE_LAB=yes -v ~/:/home/jovyan/work data-analyst-notebook There is more detailed instruction from User Guide on ReadTheDocs\nIf you feel like that the command is too long to run. You can add an alias to your .bashrc file like this:\nalias dslab=\u0026#39;docker run -p 8888:8888 -e JUPYTER_ENABLE_LAB=yes -v ~/:/home/jovyan/work data-analyst-notebook\u0026#39; Now you can use dslab in the terminal as a replacement for typing the long command.\n","date":"2019-03-20T02:02:28.828Z","image":"https://lijohnny.com/p/data-analysis-with-docker/5cc38cc0066e1_hu70d1880b8c0962ac8fd85c3141778403_516435_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/data-analysis-with-docker/","title":"Data Analysis with Docker"},{"content":"In this tutorial I use Scrapy to collect data from Craigslist.com. Specifically, the data under craigslist.org/Seattle/housing/vacation rentals. You can find the page under the link: https://seattle.craigslist.org/d/vacation‐rentals/search/vac\nIn the example, I collected following information:\n Title Posted Date Rental Price Number of bedrooms Neighborhood Description   For more information or the code, please go to my github page.\n PREPARATION INSTALLATION You can install scrapy through pip install command:\n$ pip install scrapy or use conda install command:\n$ conda install scrapy CREAT PROJECT Before we start coding, we can use scrapy startproject command to quickly create a project. In terminal or CMD, navigate to your desired folder and execute following command:\n$ scrapy startproject scrapy_craigslist Here scrapy_craigslist is the name of the project.\nAfter that, we can use genspider command to create a Scrapy Spider. Here, we name it vacation_rentals and designated a URL. We user craiglist.org Seattle vacation house list page as an example.\n$ scrapy genspider vacation_rentals seattle.craigslist.org/d/vacation‐rentals/search/vac This will create a directory with the following structure:\n─── scrapy_craigslist ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── settings.cpython-36.pyc ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py └── spiders ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── vacation_rentals.cpython-36.pyc └── vacation_rentals.py EDITING Navigate to the spiders folder and open the spider py file in your favorite editor. There are some pre written code, but you need to make sure that allowed_domains and start_urls are in the right form.\nimport scrapy class CarSpider(scrapy.Spider): name = \u0026#39;car\u0026#39; allowed_domains = [\u0026#39;craigslist.org\u0026#39;] start_urls = [\u0026#39;https://seattle.craigslist.org/d/vacation‐rentals/search/vac/\u0026#39;] def parse(self, response): pass Let\u0026rsquo;s write our own code under def parse(self, response):. You can check the code here.\n# -*- coding: utf-8 -*- import scrapy from scrapy import Request import re class VacationRentalsSpider(scrapy.Spider): name = \u0026#39;vacation_rentals\u0026#39; allowed_domains = [\u0026#39;craigslist.org\u0026#39;] start_urls = [\u0026#39;http://seattle.craigslist.org/d/vacation‐rentals/search/vac/\u0026#39;] def parse(self, response): # Extract all wrapper for each list item between \u0026lt;p class=\u0026#34;result-info\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; vacs = response.xpath(\u0026#39;//p[@class=\u0026#34;result-info\u0026#34;]\u0026#39;) # Get next page button URL \u0026lt;a href=\u0026#34;/search/vac?s=120\u0026#34; class=\u0026#34;button next\u0026#34; title=\u0026#34;next page\u0026#34;\u0026gt;next \u0026amp;gt; \u0026lt;/a\u0026gt; next_rel_url = response.xpath(\u0026#39;//a[@class=\u0026#34;button next\u0026#34;]/@href\u0026#39;).extract_first() # Get full address. next_url = response.urljoin(next_rel_url) # Go through all the pages. yield Request(next_url, callback=self.parse) # Loop each item to extract title, posted date, rental price, number of bedrooms, and neighborhood for vac in vacs: # Get title from \u0026lt;a\u0026gt;\u0026lt;/a\u0026gt; tag. title = vac.xpath(\u0026#39;a/text()\u0026#39;).extract_first() # Get posted date from \u0026lt;time class=\u0026#34;result-date\u0026#34; datetime=\u0026#34;2019-03-06 18:34\u0026#34; title=\u0026#34;Wed 06 Mar 06:34:28 PM\u0026#34;\u0026gt;Mar 6\u0026lt;/time\u0026gt; block. Use @datetime for attribute datetime. pdate = vac.xpath(\u0026#39;time/@datetime\u0026#39;).extract_first().split()[0] # Get rental price form \u0026lt;span class=\u0026#34;result-price\u0026#34;\u0026gt;$84\u0026lt;/span\u0026gt; rprice = vac.xpath(\u0026#39;span/span[@class=\u0026#34;result-price\u0026#34;]/text()\u0026#39;).extract_first() # Get Number of bedrooms from \u0026lt;span class=\u0026#34;housing\u0026#34;\u0026gt;2br - 760ft\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt; - \u0026lt;/span\u0026gt; and clean up the extra nbedroom = str(vac.xpath(\u0026#39;span/span[@class=\u0026#34;housing\u0026#34;]/text()\u0026#39;).extract_first()).split(\u0026#39;-\u0026#39;)[0].strip() # Get Neighborhood from \u0026lt;span class=\u0026#34;result-hood\u0026#34;\u0026gt; (*** - *****)\u0026lt;/span\u0026gt; hood = re.sub(\u0026#39;[()]\u0026#39;, \u0026#39;\u0026#39;, str(vac.xpath(\u0026#39;span/span[@class=\u0026#34;result-hood\u0026#34;]/text()\u0026#39;).extract_first())).strip() # Get the address of description page of each vacation house. vacaddress = vac.xpath(\u0026#39;a/@href\u0026#39;).extract_first() # We needed open the URL of each house and scrape the house description, while passing the meta to parse_page function. yield Request(vacaddress, callback=self.parse_page, meta={\u0026#39;URL\u0026#39;: vacaddress, \u0026#39;Title\u0026#39;: title, \u0026#39;Posted Date\u0026#39;:pdate,\u0026#34;Rental Price\u0026#34;:rprice,\u0026#34;Number of bedrooms\u0026#34;:nbedroom, \u0026#34;Neighborhood\u0026#34;:hood}) # Extract description page of the vacation house. def parse_page(self, response): # Pass the variables url = response.meta.get(\u0026#39;URL\u0026#39;) title = response.meta.get(\u0026#39;Title\u0026#39;) pdate = response.meta.get(\u0026#39;Posted Date\u0026#39;) rprice = response.meta.get(\u0026#39;Rental Price\u0026#39;) nbedroom = response.meta.get(\u0026#39;Number of bedrooms\u0026#39;) hood = response.meta.get(\u0026#39;Neighborhood\u0026#39;) # Get the description. description = \u0026#34;\u0026#34;.join(line for line in response.xpath(\u0026#39;//*[@id=\u0026#34;postingbody\u0026#34;]/text()\u0026#39;).extract()) yield{\u0026#39;Title\u0026#39;: title, \u0026#39;Posted Date\u0026#39;:pdate,\u0026#34;Rental Price\u0026#34;:rprice,\u0026#34;Number of bedrooms\u0026#34;:nbedroom, \u0026#34;Neighborhood\u0026#34;:hood,\u0026#39;Description\u0026#39;:description} RUN SPIDER To put our spider to work, run crawl command in terminal or CMD:\n$ scrapy crawl vacation_rentals -o result-titles.csv -o means out put data into file. result-titles.csv is the files' name.\n","date":"2019-02-12T01:13:34-08:00","image":"https://lijohnny.com/p/craigslist-vacation-house-data-crawling/5cd7d789264a1_huc7cb634be90cb868cb769f9003ac1e34_106268_120x120_fill_q75_box_smart1.jpg","permalink":"https://lijohnny.com/p/craigslist-vacation-house-data-crawling/","title":"Craigslist Vacation House Data Crawling"}]