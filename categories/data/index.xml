<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data on Johnny's</title><link>https://lijohnny.com/categories/data/</link><description>Recent content in Data on Johnny's</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 30 May 2019 01:03:50 +0000</lastBuildDate><atom:link href="https://lijohnny.com/categories/data/index.xml" rel="self" type="application/rss+xml"/><item><title>Donors Identification for CharityML</title><link>https://lijohnny.com/p/donors-identification-for-charityml/</link><pubDate>Thu, 30 May 2019 01:03:50 +0000</pubDate><guid>https://lijohnny.com/p/donors-identification-for-charityml/</guid><description>&lt;p>&lt;a class="link" href="https://lijohnny.com" target="_blank" rel="noopener"
>&lt;img src="https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg" alt="" />&lt;/a> &lt;a class="link" href="https://www.python.org/" target="_blank" rel="noopener"
>&lt;img src="https://img.shields.io/badge/Made%20with-Python-1f425f.svg" alt="" />&lt;/a> &lt;a class="link" href="" >&lt;img src="https://img.shields.io/badge/Kaggle-Project-blue.svg" alt="" />&lt;/a>&lt;/p>
&lt;p>In this project, I built machine learning models that best identifies potential donors for CharityML(a fictitious charity organization) with data collected for the U.S. census. To find the best approach, I performed EDA, feature engineering, and building training and predicting pipeline to evaluate and optimize the performance between different machine learning models.&lt;/p>
&lt;p>The modified census dataset consists of approximately 32,000 data points, with each datapoint having 13 features. This dataset is a modified version of the dataset published in the paper &lt;em>&amp;ldquo;Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid&amp;rdquo;,&lt;/em> by Ron Kohavi. You may find this paper &lt;a class="link" href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf" target="_blank" rel="noopener"
>online&lt;/a>, with the original dataset hosted on &lt;a class="link" href="https://archive.ics.uci.edu/ml/datasets/Census&amp;#43;Income" target="_blank" rel="noopener"
>UCI&lt;/a>.&lt;/p>
&lt;p>The model I have used:&lt;/p>
&lt;ul>
&lt;li>SGD Classifier&lt;/li>
&lt;li>AdaBoost&lt;/li>
&lt;li>Logistic Regression&lt;/li>
&lt;/ul></description></item><item><title>Spam Detection and Analysis (Part 2 - Analysis with R)</title><link>https://lijohnny.com/p/spam-detection-and-analysis-part-2-analysis-with-r/</link><pubDate>Thu, 09 May 2019 21:47:43 +0000</pubDate><guid>https://lijohnny.com/p/spam-detection-and-analysis-part-2-analysis-with-r/</guid><description>Spam is still a common attack method. Most of the email services have spam filters that can help us block and filter out most of the emails with commercial, fraudulent and malicious content. The purpose of the project is to explore and analyze the email header to identify the features that can tell us which emails are malicious.
This is the Part 2 of the series. For how to parsing email, please check Part 1</description></item><item><title>Spam Detection and Analysis (Part 1 - Parsing)</title><link>https://lijohnny.com/p/spam-detection-and-analysis-part-1-parsing/</link><pubDate>Fri, 19 Apr 2019 01:24:59 +0000</pubDate><guid>https://lijohnny.com/p/spam-detection-and-analysis-part-1-parsing/</guid><description>Spam is still a common attack method. Most of the email services have spam filters that can help us block and filter out most of the emails with commercial, fraudulent and malicious content. The purpose of the project is to explore and analyze the email header to identify the features that can tell us which emails are malicious.
Get Email Header information First thing we need to do is the get the data.</description></item><item><title>How to Build Consistent Development Environment through Docker</title><link>https://lijohnny.com/p/how-to-build-consistent-development-environment-through-docker/</link><pubDate>Wed, 20 Mar 2019 02:02:28 +0000</pubDate><guid>https://lijohnny.com/p/how-to-build-consistent-development-environment-through-docker/</guid><description>Data Analysis is not all about reports or visualization. The correctness and reproducibility are also important for scientific research. A consistent environment is critical for reproducibility. There are several ways to achieve that. However, I find out using Docker at any time can repeat the experiment in the same environment. It is easy to scale up and scale horizontally. For more information, you can click here.
Docker File A Dockerfile basically &amp;ldquo;tells&amp;rdquo; docker the way an image build.</description></item><item><title>Craigslist Vacation House Data Crawling</title><link>https://lijohnny.com/p/craigslist-vacation-house-data-crawling/</link><pubDate>Tue, 12 Feb 2019 01:13:34 -0800</pubDate><guid>https://lijohnny.com/p/craigslist-vacation-house-data-crawling/</guid><description>&lt;p>In this &lt;a class="link" href="https://github.com/iamjohnnyli/web-crawler-tutorial/tree/master/scrapy_craigslist" target="_blank" rel="noopener"
>tutorial&lt;/a> I use &lt;a class="link" href="https://scrapy.org/" target="_blank" rel="noopener"
>Scrapy&lt;/a> to collect data from Craigslist.com. Specifically, the data under craigslist.org/Seattle/housing/vacation rentals. You can find the page under the link: &lt;a class="link" href="https://seattle.craigslist.org/d/vacation%e2%80%90rentals/search/vac" target="_blank" rel="noopener"
>https://seattle.craigslist.org/d/vacation‐rentals/search/vac&lt;/a>&lt;/p></description></item></channel></rss>